#!/usr/bin/perl -T
use strict;
use warnings;
use Time::HiRes qw( time );
use Encode;
use Data::Dumper;
use Fcntl qw(:flock SEEK_END);
# use List::Util 'shuffle';
use Cwd;
my $pwd = getcwd;
my $username = getpwuid($<);
my $cachedir = "/cgi-app-cache/search-files";  # ugly hack: these two dirs are OWNED and writable only by www-data

my $tm_start = time();

my $et_srch = 0;
my $et_find = 0;

$ENV{PATH} = '/bin:/usr/bin';  # to allow running under 'taint mode' (-T in shebang)

# testing cmdlines using http://perldoc.perl.org/CGI.html#DEBUGGING
# time ./search-files search_scope=b search_keys='nginx EPUB'

# see   STDERR -> /var/log/nginx/error.log for errors executing this script (ubuntu-specific?)
# print STDERR "\npwd=$pwd\nusername=$username\nHOME=$ENV{HOME}\n";

# runtime const behavioral controls:
my $showRegex = 0;
my $count_uniq_cands   = 0;  # has major performance impact
my $count_uniq_matches = 1;  # has no    performance impact

sub mtime { -f $_[0] ? (stat($_[0]))[9] : 0 ; }

my $maxCopyrightYr = do {
   my ($sec,$min,$hour,$mday,$mon,$year,$wday,$yday,$isdst) = localtime();
   $year = $year+1900;  # std xlation
   $year + 1;  # (c)Y+1 books appear late in year Y
   };

my %tok_remap = ( and => '(?:N|AND)', 'c#' => 'CSHARP', 'f#' => 'FSHARP' );

my ($bound_re_before,$bound_re_after) = ( '(?:^|[\W_])', '(?:$|[\W_])' ); # can't use \b because $qm may not consist of word chars only
my $bound_re = '(?:\b|[_])';
sub gen_re_match_all_anyorder {
   my @reraw;
   for (@_) {  # construct regex matching lines containing, in any order, ALL of @_
      my $qm = quotemeta( $_ );            #  match fragment
      $qm = $tok_remap{lc $qm} if exists $tok_remap{lc $qm};
      $qm = "$bound_re_before$qm$bound_re_after" if ($qm =~ m=[A-Z]=) && ! ($qm =~ m=[a-z]=); # all caps? match word
      push @reraw, "(?=.*$qm)"; # https://stackoverflow.com/questions/4389644/regex-to-match-string-containing-two-names-in-any-order
      }
   my $rv = '^(?i)' . join('',@reraw) . '.*$'; # print "\npat=$rv\n";
   return qr/$rv/;
   }

sub search_files { my ($qy,$isa,$fsroot,$webroot,$findtype,$pat) = @_;
   # print STDERR "fsroot=$fsroot\n";
   my ($fsrt_tail) = $fsroot =~ m|([^/]+)$|;
   # print STDERR "fsrt_tail=$fsrt_tail\n";
   my $findoutfnm = "$cachedir/$fsrt_tail";
   {  # $semfh is used in case fcgiwrap does not single-thread CGI processes;
      # this is not tremendously efficient in case of collision, but not awful either
      # _could_ fail flock immediately (LOCK_NB) and try a different $fsroot to parallel-process find-checking (and worst-case: scanning)
      my $semfh;
      if( 1 ) {
         my $semfnm = "$cachedir/$fsrt_tail.sem";
         open $semfh, '>', $semfnm or die "abend: cannot open $semfnm for writing: $!\n";
         flock( $semfh, LOCK_EX )  or die "abend: flock $semfnm failed: $!\n";
         }
      my $modlogfnm = "$fsroot/.modify.log";
      my ($mtime_modlog , $mtime_findout) = (mtime($modlogfnm),mtime($findoutfnm));
      if( $mtime_modlog < $mtime_findout ) {
         # print STDERR "HIT! $findoutfnm newer than $modlogfnm\n";
         }
      else {
         my $findcmd = "cd $fsroot && find . -type $findtype > $findoutfnm";
         # print STDERR "running '$findcmd'\n";
         my $t_find_start = time();
         system( $findcmd );
         $et_find += time() - $t_find_start;
         }
      close $semfh if $semfh;
   }
   my ($cands, $matches) = (0, 0);
   my (%unique_cands,%unique_matches);
   my (%rv,%ft_f_keys,%ext_variant);
   my $split_fnm_base_type = ($findtype eq 'd')
      ? sub { ($_[0],''); }
      : sub { ($_[0] =~ m=(.+?)((?:[_.][Cc]ode|\.medtype|_cropped)?\.[^\.]+)$=); } ;
   open my $ifh, '<', $findoutfnm or die "abend: cannot open $findoutfnm for reading: $!\n";
   my $t_srch_start = time();
   while ( my $line = <$ifh> ) {
      chomp $line;
      my ($fnm) = $line =~ m=([^/]+)$=; # print "$fnm\n";
      my $tgt = ".$fnm.";
      my ($base,$type);
      if( $count_uniq_cands ) {
         ($base,$type) = $split_fnm_base_type->($fnm);
         ++$unique_cands{$base};
         }
      ++$cands;
      if( $tgt =~ m,$pat, ) { # print "$tgt\n" ;
         ($base,$type) = $split_fnm_base_type->($fnm) unless $base;
         ++$unique_matches{$base} if $count_uniq_matches;
         ++$matches;
         my $yr = do {
            my @yr4s = $tgt =~ m=$bound_re(\d{4})$bound_re=g; # normally, (c) year is given as yyyy
            my @more = $tgt =~ m=$bound_re(\d{4})\d{2}$bound_re=g;   push( @yr4s, @more     ); # but rarely I give yyyymm
               @more = $tgt =~ m=$bound_re(\d{4})\d{4}$bound_re=g;   push( @yr4s, @more     ); # and rarely I give yyyymmdd
            my ($yr2)= $tgt =~ m=$bound_re(\d{2})\.\d{2}$bound_re=g; push( @yr4s, $yr2+2000 ) if $yr2; # more rarely I give yy.mm
            my $rv = '';
            if( @yr4s ) {
               my $max = 0;
               $max = ($_<=$maxCopyrightYr && $_>$max) ? $_ : $max foreach @yr4s;  # print $max,"\n";
               $rv = $max if $max > 0;
               }
            $rv;
            };
         my $ofnm = "$fsroot/$line"; $ofnm =~ s=/\./=/=; # print "$ofnm\n" ;
         my $link; ($link = $ofnm) =~ s=^$fsroot=$webroot=; # print "link=$link\n";
         if( $findtype eq 'd' || ! $type ) {
            my $reldir; ($reldir = $link) =~ s=^$webroot/==;
            push @{$rv{$yr}}, $qy->a({href=>"$link"},$reldir);
            }
         else {
            my $key = substr( $link, 0, - length $type );
            $ft_f_keys{$key} = $yr;
            push @{$ext_variant{$key}}, $type;
            }
         }
      }
   for my $link_wo_ext (sort keys %ft_f_keys) {  # fold ext_variants into rv
      my @out = $link_wo_ext =~ m=([^/]+)$=;
      my $lwx = $link_wo_ext =~ s{([#])}{sprintf("%%%02X",ord($1))}egr;  # poor man's urlencode (for '#' only FTTB)
      for my $ext ( reverse sort @{$ext_variant{$link_wo_ext}} ) {  # reverse sort is a marginal first approximation of the 'order of [type] preference' described above
         push @out, $qy->a({href=>"$lwx$ext"},$ext),"\n";
         }
      my $yr = $ft_f_keys{$link_wo_ext};
      push @{$rv{$yr}}, join(' ',@out);
      }
   $et_srch += time() - $t_srch_start;
   return (\%rv,
            scalar keys %unique_cands   || $cands  ,
            scalar keys %unique_matches || $matches,
          );
   }

#
# As noted in README.md
#    * suffix variants: `2005.ext` might morph to either
#        * `2005.medtype.ext` generated by [Calibre](https://calibre-ebook.com/ ) when creating PDF from non-PDF, or
#        * `2005_cropped.ext` generated by [briss2](https://github.com/fwmechanic/briss2 ) when cropping a PDF.
# the vast majority of the time, a user will want to download the (cropped) pdf
# version of the ebook.  Since there are potentially many variants (formats and
# croppings) of the SAME book (in INCREASING order of preference:
#    * book.(chm|djvu|azw3|mobi|epub)
#    * book.azw3
#    * book.mobi
#    * book.epub
#    * book.pdf
#    * book_cropped.pdf  (locally generated from book.pdf)
#    * book.medtype.pdf  (locally generated from book.(chm|azw3|mobi|epub))
# ) it would be helpful for these to collapse into a single line containing
# multiple links, with the best version ( being left-most (and the longest
# link).
#
# What I think this means is I want to determine the basename (book) and find
# all files having the same basename and compress them as above.
#
use CGI;
my $qy = CGI->new; my $norm_search_terms;
print $qy->header;
print $qy->start_html(
      -title=>$norm_search_terms,
      -base=>'true', -target=>'_blank',  # links on this page will open in new client (browser) tab/window.
      );

my @treelocns = (  # unfortunately necessary hardcoding of app filesys/webapp mappings
   # align with /etc/nginx/sites-enabled/default (must edit as root)
   { isa => 'b', cat => 'Books'     , fsroot=>'/mnt/smb/5t_a/data/ebooks'         , webroot=> '/files/ebooks'          , ft=>'f' },
   { isa => 'm', cat => 'Music'     , fsroot=>'/mnt/smb/5t_a/data/MP3'            , webroot=> '/files/mp3'             , ft=>'d' },
   { isa => 'a', cat => 'Audiobooks', fsroot=>'/mnt/smb/5t_a/data/audiobooks'     , webroot=> '/files/audiobooks'      , ft=>'f' },
   { isa => 'v', cat => 'Videos'    , fsroot=>'/mnt/smb/5t_a/data/Video'          , webroot=> '/files/video-downloads' , ft=>'d' },
   );
my ($cands,$matches) = (0, 0);

{
my $isaset= $qy->Vars->{search_scope} // 'bavm'; # default to Book search
my $skeys = $qy->Vars->{search_keys} // '';
   # print(sprintf("%v04X", $skeys), "\n");
   # {
   # local $Data::Dumper::Useqq = 1;
   # print( decode_utf8(Dumper($skeys)) );
   # print( decode_utf8(encode_utf8($skeys)) );
   # }
   $skeys =~ s|&#9830;| |g; # remove nasty black-diamond-suit char that is injected when user hits 'back' in browser.
   $skeys =~ s='==g; # ' mimic collection-maintenance file rename
my @search_keys = split( qr{[-\s,.:"]+}, $skeys ); # " preserve: [+#]
$norm_search_terms = join(' ',@search_keys);
my $pat = gen_re_match_all_anyorder( @search_keys );
print "$pat\n" if $showRegex;

# WIP to allow processes colliding on flock $semfh to try the set of sem files in random/shuffle order
# this would also require separating find-phase from search phase as the following should seek to complete all find-phases before searching
#
# my @tlixs;
# for my $ix (0 .. $#treelocns) {
#    my ($isa,$fsroot,$webroot,$findtype) = @{$treelocns[$ix]}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
#    next unless $isa =~ m=[$isaset]=;
#    push @tlixs, $ix;
#    }
# @tlixs = shuffle(@tlixs);
# while( scalar @tlixs ) {
#    for my $ix (0 .. $#tlixs) {
#       for my $hr ( @tlixs ) {
#          my ($isa,$fsroot,$webroot,$findtype) = @{$hr}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
#          my $matches = search_files($qy,$isa,$fsroot,$webroot,$findtype,$pat);
#          if( $matches ) {
#             $hr->{matches} = $matches;
#
#             }
#          }
#       }
#    }

for my $hr ( @treelocns ) {
   my ($isa,$fsroot,$webroot,$findtype) = @{$hr}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
   next unless $isa =~ m=[$isaset]=;
   my ($ccnt, $mcnt); ($hr->{matches}, $ccnt, $mcnt) = search_files($qy,$isa,$fsroot,$webroot,$findtype,$pat);
   $cands   += $ccnt;
   $matches += $mcnt;
   }
}
# print $qy->Dump();
print $qy->h3("Found",($matches||"no"),"matches of '$norm_search_terms' among",$cands,"candidates",$qy->a({href=>"/"},"new search")), "\n";
for my $hr ( @treelocns ) {
   my $ms = $hr->{matches};
   if( scalar keys %{$ms} ) {
      my $cat = $hr->{cat};
      print $qy->a({href=>"#$cat"},$cat),"\n";
      }
   }
for my $hr ( @treelocns ) {
   my $ms = $hr->{matches};
   if( scalar keys %{$ms} ) {
      my $anch = { id => $hr->{cat} };
      for my $yr (reverse sort keys %{$ms}) { # print "$yr:\n";
         print $qy->h3( $anch, $hr->{cat} . " &copy;".($yr || "<i>unknown</i>") ), "\n";
         $anch = {};
         print $qy->ol( map { $qy->li( $_ ); } sort @{$ms->{$yr}} ), "\n";
         }
      }
   }
sub pr_et_us { return sprintf("%.6f", $_[0]); }
print $qy->h3( "Server response timing:" );
print $qy->ol( [ "Toverall: ".pr_et_us( time() - $tm_start ), " Tfind: ".pr_et_us( $et_find ), " Tsrch: ".pr_et_us( $et_srch ) ] ), "\n";
print $qy->end_html;
exit 0;
