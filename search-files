#!/usr/bin/perl -T
# -T == taint mode: https://perldoc.perl.org/perlsec#Laundering-and-Detecting-Tainted-Data

use strict;
use warnings;
use Time::HiRes qw( time );
use Encode;
use English;
use Data::Dumper;
use File::Find;
use File::Temp qw( tempdir );
use Fcntl qw(:flock SEEK_END);
use IO::Compress::Zip qw(zip $ZipError :constants);
# ^^^-end core Perl modules, vvv-begin non-core modules:
use CGI;  # from `apt-get install libcgi-pm-perl`
use CGI::Carp qw(fatalsToBrowser warningsToBrowser set_message);  # from `apt-get install libcgi-pm-perl`

# see STDERR -> /var/log/nginx/error.log for errors executing this script (ubuntu-specific?)
# testing cmdlines using http://perldoc.perl.org/CGI.html#DEBUGGING
# time ./search-files search_scope=b search_keys='nginx EPUB'
#
# use Cwd;
# my $pwd = getcwd;
# my $username = getpwuid($<);
# print STDERR "\npwd=$pwd\nusername=$username\nHOME=$ENV{HOME}\n";

use constant cachedir => "/cgi-app-cache/search-files";  # in lieu of config file or parameter support: these two dirs are OWNED and writable only by www-data
use constant common_fsroot => '/var/www-filesearcher-data';  # contains symlinks to actual file trees

# mode controls:
use constant showRegex => 0;
use constant count_uniq_cands   => 0;  # has major performance impact
use constant count_uniq_matches => 1;  # has no    performance impact

$ENV{PATH} = '/bin:/usr/bin';  # to allow running under 'taint mode' (-T in shebang)

my ($tm_start,$et_srch,$et_find) = (time(), 0, 0);

sub mtime { -f $_[0] ? (stat($_[0]))[9] : 0 ; }

use constant maxCopyrightYr => do {
   my ($sec,$min,$hour,$mday,$mon,$year,$wday,$yday,$isdst) = localtime();
   $year = $year+1900;  # std xlation
   $year + 1;  # (c)Y+1 books appear late in year Y
   };

use constant q1 => "'";
use constant q2 => '"';
use constant bound_re => '(?:\b|[_])' ;  # https://dev.to/kirklewis/string-interpolation-of-constants-in-perl-5-181o

sub gen_re_match_all_anyorder {
   # https://www.perlmonks.org/?node_id=308753 (thread: https://www.perlmonks.org/?node_id=308744)
   # to make a specialized version of \b that views "-" and "/" as "word characters" (sort of), you might use something like this:
   # my $w = '\w/-';
   my $w = shift or die "gen_re_match_all_anyorder: no args?";
   my $b = "(?:(*negative_lookbehind:[$w])(*positive_lookahead:[$w])|(*positive_lookbehind:[$w])(*negative_lookahead:[$w]))";
   # my @words = ($rec =~ /${b}[$w]+${b}/g);

   # The following implements a brute-force solution to the performance problem
   # caused by use of \b (or $b, the specialized version of \b) to implement
   # whole-word search terms:
   #
   # The returned search regex consists of a sequence of look-ahead-assertions
   # (LAA), each matching one user search term.  Previously, a word search term
   # would be wrapped in \b's before being inserted in the (one and only) LAA
   # sequence in input (i.e. user-provided) order, however the presence of \b
   # (caused by the presence of ANY word search term) caused a huge (100% == 2x)
   # performance hit.
   #
   # This is resolved by adding _TWO_ LAA's into the returned LAA sequence for
   # each input word search term:
   # 1. a non-word-search (string) LAA is added to @strs for every search term.
   # 2. a word-search (\b) LAA is added to @words for every word search term.
   #
   # The returned sequence of LAA's is the concatenation of @strs followed by
   # @words.  Thus all candidate strings must FIRST pass all @strs LAA's; these
   # are very fast checks.  Only those few candidate strings passing all @strs
   # LAA's undergo checking against the (SLOW) @words LAA's.  Assuming only a low
   # percentage of candidate strings pass all @strs LAA's, the performance impact
   # of the trailing (SLOW) @words LAA's is reduced to almost nothing (and
   # testing shows this to be true).
   #
   # Given that our search is optimized by failing each candidate string as
   # quickly as possible, a further optimization is to sort @strs LAA's by
   # descending length: this causes the longest strings among the input search
   # terms to be searched for first.  The naive idea being that longer strings
   # are less likey to be found than short strings.
   #
   my (@strs,@words);
   for my $rawterm (@_) {  # construct regex matching lines containing, in any order, ALL of @_
      $rawterm =~ s|([CF])#|\1SHARP|;
      $rawterm =~ s|([cf])#|\1sharp|;
      my $term = quotemeta( $rawterm );  #  match term
      push @strs,  "(*positive_lookahead:.*$term)"; # https://stackoverflow.com/a/4389683 https://stackoverflow.com/questions/4389644/regex-to-match-string-containing-two-names-in-any-order
      push @words, "(*positive_lookahead:.*$b$term$b)" if ($term =~ m=[A-Z]=) && ($term !~ m=[a-z]=); # all term alphas are caps (i.e. at least one uppercase-alpha and no lowercase-alphas)?: match word as defined by $b + $w
      }
   @strs = sort { length $b <=> length $a } @strs;  # try to find longest strings first
   my $rv = '^(?i)' . join('', @strs) . join('', @words) . '.*$'; # print "\npat=$rv\n";
   return qr($rv);
   }

sub extract_yr { my($tgt) = @_;
   my @yr4s = $tgt =~ m=${\bound_re}(\d{4})${\bound_re}=g; # normally, (c) year is given as yyyy
   my @more = $tgt =~ m=${\bound_re}(\d{4})\d{2}${\bound_re}=g;   push( @yr4s, @more     ); # but rarely I give yyyymm
      @more = $tgt =~ m=${\bound_re}(\d{4})\d{4}${\bound_re}=g;   push( @yr4s, @more     ); # and rarely I give yyyymmdd
   my ($yr2)= $tgt =~ m=${\bound_re}(\d{2})\.\d{2}${\bound_re}=g; push( @yr4s, $yr2+2000 ) if $yr2; # more rarely I give yy.mm
   my $rv = '';
   if( @yr4s ) {
      my $max = 0;
      $max = ($_<=maxCopyrightYr && $_>$max) ? $_ : $max foreach @yr4s;  # print $max,"\n";
      $rv = $max if $max > 0;
      }
   $rv;
   }

sub search_treelocn { my ($qy,$isa,$fsroot,$webroot,$findtype,$pat,$su) = @_;
   # print STDERR "fsroot=$fsroot\n";
   my ($fsrt_tail) = $fsroot =~ m|([^/]+)$|;
   # print STDERR "fsrt_tail=$fsrt_tail\n";
   my $findoutfnm = cachedir."/$fsrt_tail";
   {  # $semfh is used in case fcgiwrap does not single-thread CGI processes;
      # this is not tremendously efficient in case of collision, but not awful either
      # _could_ fail flock immediately (LOCK_NB) and try a different $fsroot to parallel-process find-checking (and worst-case: scanning)
      my $semfh;
      if( 1 ) {
         my $semfnm = cachedir."/$fsrt_tail.sem";
         open $semfh, '>', $semfnm or die "abend: cannot open $semfnm for writing: $!\n";
         flock( $semfh, LOCK_EX )  or die "abend: flock $semfnm failed: $!\n";
         }
      my $modlogfnm = "$fsroot/.modify.log";
      my ($mtime_modlog , $mtime_findout) = (mtime($modlogfnm),mtime($findoutfnm));
      if( $mtime_modlog < $mtime_findout ) {
         # print STDERR "HIT! $findoutfnm newer than $modlogfnm\n";
         }
      else {
         my $ft = $findtype; $ft .= ',l' if $ft =~ m|f|;
         my $findcmd = "cd '$fsroot' && find . -type '$ft' > '$findoutfnm'";
         # print STDERR "running '$findcmd'\n";
         my $t_find_start = time();
         system( $findcmd );
         $et_find += time() - $t_find_start;
         }
      close $semfh if $semfh;
   }
   my ($cands, $matches) = (0, 0);
   my (%unique_cands,%unique_matches);
   my (%rv,%ft_f_keys,%extsOf);
   my $split_fnm_base_ext = ($findtype eq 'd')
      ? sub { ($_[0],''); }   # note that WE USE AN EXTENDED DEFINITION OF "ext" as defined HERE_EXT
      : sub { ($_[0] =~ m=(.+?)((?:[_.][Cc]ode|\.medtype|_cropped)?\.[^\.]+)$=); } ;  # <-- HERE_EXT
   my $t_srch_start = time();
   {
   open my $ifh, '<', $findoutfnm or die "abend: cannot open $findoutfnm for reading: $!\n";
   while ( my $fsNmRel = <$ifh> ) {  # one filename or dirname (relative to $fsroot) per line
      chomp $fsNmRel;
      my ($fnm) = $fsNmRel =~ m=([^/]+)$=; # print "$fnm\n";
      my $tgt = ".$fnm.";
      my ($base,$ext);
      if( count_uniq_cands ) {
         ($base,$ext) = $split_fnm_base_ext->($fnm);
         ++$unique_cands{$base};
         }
      ++$cands;
      if( $tgt =~ m,$pat, ) { # print "$tgt\n" ;  # MATCH!  slow path
         ($base,$ext) = $split_fnm_base_ext->($fnm) unless $base;
         ++$unique_matches{$base} if count_uniq_matches;
         ++$matches;
         my $yr = extract_yr( $tgt );
         my $fsNmAbs = "$fsroot/$fsNmRel" =~ s=/\./=/=r; # print "$fsNmAbs\n" ;
         my $weblink = $fsNmAbs =~ s=^$fsroot=$webroot=r; # print "weblink=$weblink\n";
         if( $findtype eq 'd' || ! $ext ) {
            my $reldir = $weblink =~ s=^$webroot/==r;
            push @{$rv{$yr}}, $qy->a({href=>"$weblink"},$reldir) .' '. $qy->a({href=>"$su?zipdowndirnm=$weblink"},'zipdown!');
            }
         else {
            my $link_wo_ext = substr( $weblink, 0, - length $ext );
            $ft_f_keys{$link_wo_ext} = $yr;
            push @{$extsOf{$link_wo_ext}}, $ext;
            }
         }
      }
   } # closes $ifh
   for my $link_wo_ext (sort keys %ft_f_keys) {  # fold extsOf into rv
      my @out = $link_wo_ext =~ m=([^/]+)$=;
      my $lwx = $link_wo_ext =~ s{([#])}{sprintf("%%%02X",ord($1))}egr;  # poor man's urlencode (for '#' only FTTB)
      for my $ext ( sort { $b cmp $a } @{$extsOf{$link_wo_ext}} ) {  # reverse sort is a marginal first approximation of the 'order of [type] preference' described above
         push @out, $qy->a({href=>"$lwx$ext"},$ext),"\n";
         }
      my $yr = $ft_f_keys{$link_wo_ext};
      push @{$rv{$yr}}, join(' ',@out);
      }
   $et_srch += time() - $t_srch_start;
   return (\%rv,
            scalar keys %unique_cands   || $cands  ,
            scalar keys %unique_matches || $matches,
          );
   }

#
# As noted in README.md
#    * suffix variants: `2005.ext` might morph to either
#        * `2005.medtype.ext` generated by [Calibre](https://calibre-ebook.com/ ) when creating PDF from non-PDF, or
#        * `2005_cropped.ext` generated by [briss2](https://github.com/fwmechanic/briss2 ) when cropping a PDF.
# the vast majority of the time, a user will want to download the (cropped) pdf
# version of the ebook.  Since there are potentially many variants (formats and
# croppings) of the SAME book (in INCREASING order of preference:
#    * book.(djvu|azw3|chm|mobi|epub|pdf)
#    * book_cropped.pdf  (locally generated from book.pdf)
#    * book.medtype.pdf  (locally generated from book.(azw3|chm|mobi|epub))
#    * book.code.zip     (git repo(s) associated with book text)
# ) it would be helpful for these to collapse into a single line containing
# multiple links, with the best version ( being left-most (and the longest
# link).
#
# What I think this means is I want to determine the basename (book) and find
# all files having the same basename and compress them as above.
#
my @treelocns = (  # unfortunately necessary hardcoding of app filesys/webapp mappings
   # HACK ALERT: handler_search_keys call tree adds {matches} to these array entries!!!
   # align with /etc/nginx/sites-enabled/default (must edit as root)
   { isa=>'b', ft=>'f', cat=>'Books'      , webroot=>'/files/ebooks'       , fsroot=>common_fsroot.'/ebooks'       },
   { isa=>'m', ft=>'d', cat=>'Music'      , webroot=>'/files/MP3'          , fsroot=>common_fsroot.'/MP3'          },
 # { isa=>'m', ft=>'d', cat=>'Music'      , webroot=>'/files/MP3_extranea' , fsroot=>common_fsroot.'/MP3_extranea' },
   { isa=>'a', ft=>'f', cat=>'Audiobooks' , webroot=>'/files/audiobooks'   , fsroot=>common_fsroot.'/audiobooks'   },
   { isa=>'v', ft=>'d', cat=>'Videos'     , webroot=>'/files/Video'        , fsroot=>common_fsroot.'/Video'        },
   );

my $qy = CGI->new;
# dispatch
sub rsp404 {
   print $qy->header( -status => '404 Not Found', );
   exit 0;
   }
if   ( exists $qy->Vars->{search_keys}  ) { handler_search_keys( $qy ); }
elsif( exists $qy->Vars->{zipdowndirnm} ) { handler_zipdownload( $qy ); }
else { rsp404(); }

sub pr_et_us { return sprintf("%.6f", $_[0]); }

sub handler_zipdownload { my ($qy) = @_;
   my $p = '<br>';
   my ($weblink) = $qy->Vars->{zipdowndirnm} =~ m=^((?:(?!\.\./).)*)$=; # print "weblink=$weblink$p\n";   # untaint
   my @dbglog;  # since if we output (html headers and) any text, we'll be unable to download the target file, we collect log text and output on failure
   my $carp_msg_handler = sub {
      my $msg = shift;
      local $LIST_SEPARATOR = "\n";
      print "$p@dbglog\n";
      print "${p}${p}Error: $msg" if $msg;
      };
   set_message( $carp_msg_handler );
   my $dl = sub { push @dbglog, join( ' ', @_ ) if @_; };
   $dl->( $qy->header );
   $dl->( $qy->start_html(
         -title=>"zipdownload $weblink",
         -base=>'true', -target=>'_blank',  # links on this page will open in new client (browser) tab/window.
         )
        );
   for my $hr ( @treelocns ) {  # linear search to find treelocn containing $weblink so we can xlat to fs locn
      my ($fsroot,$webroot,$findtype) = @{$hr}{qw(fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
      next unless $weblink =~ m=^\Q$webroot=;
      $dl->( $qy->h3("zipdownload $weblink"), "\n" );
      $dl->( "hit on webroot=$webroot" );
      $dl->( " but findtype=$findtype != 'd'" ) unless $findtype eq 'd';
      $dl->( "$p\n" );
      my $fsNmAbs = $weblink =~ s=^\Q$webroot=$fsroot=r;
      $dl->( "fsNmAbs=$fsNmAbs" );
      $dl->( " but it isn't a directory!" ) unless -d $fsNmAbs;
      $dl->( "$p\n" );

      my $td = tempdir( DIR => cachedir, CLEANUP => 1 );
      $dl->( "tempdir=$td$p\n" );

      # copied from my repo=shell/m3u
      my ($reToZip,$reM3u) =
      do {
         my @playableExts   = qw( mp3 mp4 mpg m4a m4b wmv flac f4v flv );
         my @playAssistExts = qw( srt vtt );
         sub extRE {
            my $rv = '\.(?i)(?:' . join('|', @_) . ')$'; # $dl->( "\npat=$rv\n" );
            return qr($rv);
            }
         ( extRE( @playableExts, @playAssistExts )
         , extRE( @playableExts )
         )
         };
      my $rePathLeaf = qr([^/]+$);
      my ($zipfnm) = $fsNmAbs =~ m!($rePathLeaf)!;  $zipfnm .= '.zip';
      my $reFsNmAbs = '^' . quotemeta($fsNmAbs =~ s!$rePathLeaf!!r);  $dl->( "reFsNmAbs=$reFsNmAbs$p\n" );
      my (@srcfnms,@m3ufnms,%eachdir);
      find(
         { wanted => sub {
            return unless -f $_;             # $dl->( "F0=$_\n" );                 # sanspath i.e. leaf name: might be a dir; ignore these
            ++$eachdir{$File::Find::dir};    # $dl->( "F1=$File::Find::name\n" );  # == "$File::Find::dir/$_": NB !!! WE ARE CD'D INTO $File::Find::dir !!!
            push @srcfnms, $File::Find::name if $_ =~ $reToZip;
            push @m3ufnms, $File::Find::name if $_ =~ $reM3u;
            },
         , untaint => 1  # https://perldoc.perl.org/File::Find#%25options
         , untaint_pattern => qr|^([-+@\w\s./]+)$|  # update (for other filename chars) if 'Insecure dependency in chdir while running with -T switch' is encountered
         }, $fsNmAbs
         );  # temporarily CD's into $fsNmAbs dir tree dirs;

      @srcfnms = sort @srcfnms;  # prettify for $dl; no functional need
      $dl->( "${p}srcfnms:\n" );
      { # we create zip file w/Z_NO_COMPRESSION so sum of input filesizes is an accurate predictor of zip file size
      my ($maxMB, $sumMB) = (1500, 0);
      for( @srcfnms ) {
         my $fMB = int((stat $_)[7] / (1024*1024));
         $dl->( $p, sprintf( '%3dMB ', $fMB ), $_, "\n" );
         $sumMB += $fMB;
         }
      die "total playable file size ${sumMB}MB > ${maxMB}MB limit\n" if $sumMB > $maxMB;
      $dl->( "${p}total playable file size ${sumMB}MB <= ${maxMB}MB limit\n" );
      }

      # prior to zipping generate a m3u file in eachdir _if_ it or any of its child dirs contain @m3ufnms

      # since this process does NOT have write access to $fsNmAbs, we will write
      # any generated m3u files to a tempdir, and use zip-time archive filename
      # mapping (FilterName) to include these files in the zip file with the
      # intended user-visible names
      my $tempfIx = 0;
      my %tempFnmMap;
      sub wr_array { my ($ofnm,$aref) = @_;
         my $tmpFnm = sprintf( "$td/wr_array_%03d", $tempfIx++ );
         $dl->( "${p}writing $ofnm ($tmpFnm)\n" );
         $tempFnmMap{$tmpFnm} = $ofnm;
         $ofnm              = $tmpFnm;
         open my $ofh, ">", $ofnm or die "can't open $ofnm for writing: $!\n";
         local $LIST_SEPARATOR = "\n";
         print $ofh "@$aref";
         push @srcfnms, $ofnm;
         }
      sub write_m3u { my ($destdir,$aref) = @_; # $dl->( "F0=$destdir\n" );
         my @results;
         my $ddre = quotemeta($destdir);
         for my $fnm ( @$aref ) {
            if( my ($fnm_rel) = $fnm =~ m|^$ddre[\\/](.+)$| ) { # $dl->( "F1=$fnm_rel\n" );
               $fnm_rel =~ s!/+!\\!g;  # $dl->( "F2=$fnm_rel\n" );
               $fnm_rel =~ s!\\+!\\!g; # $dl->( "F3=$fnm_rel\n" );
               $fnm_rel =~ s!^\.\\!!;  # $dl->( "F4=$fnm_rel\n" );
               push @results, $fnm_rel;
               }
            }
         if( @results > 1 ) {
            @results = sort @results;
            wr_array( "$destdir/00.m3u", \@results ) ;  # no playlist file needed for single playable file
            }
         }

      $dl->( "${p}\n" );
      write_m3u( $_, \@m3ufnms ) for ( sort keys %eachdir );
      @srcfnms = sort @srcfnms;  # merge m3u additions

      $dl->( "${p}${p}srcfnms:\n" ); $dl->( $p, $_, "\n" ) for @srcfnms;
      {
      # A VERY annoying property of map BLOCK that I forgot(?): modifying $_ in
      # BLOCK modifies the SOURCE array element that $_ is an alias for (so in
      # this case, acting on $_ directly ($_ = $tempFnmMap{$_} // $_;) causes
      # @srcfnms to be modified).
      #
      # Following are 3 ways to workaround:
      my @mappedSrcFnms = map {                  ($tempFnmMap{$_} // $_)    =~ s!$reFsNmAbs!!r } @srcfnms;  # perform return-subst on a rvalue (expression)
    # my @mappedSrcFnms = map { my $f           = $tempFnmMap{$_} // $_; $f =~ s!$reFsNmAbs!!r } @srcfnms;  # use new var containing copy of $_ value in lieu of $_
    # my @mappedSrcFnms = map { local $_=$_; $_ = $tempFnmMap{$_} // $_;       s!$reFsNmAbs!!r } @srcfnms;  # local-ize $_ https://stackoverflow.com/a/63167
      $dl->( "${p}${p}mappedSrcFnms:\n" ); $dl->( $p, $_, "\n" ) for @mappedSrcFnms;
      }

      $dl->( "${p}dope done\n" );
      $dl->( $qy->ol( [ "Toverall: ".pr_et_us( time() - $tm_start ) ] ), "\n" );

      my $absZipfnm = "$td/zipfile";  # name of this file is independent of downloaded filename

      # Create zip file by writing it to $absZipfnm, then immediately cat the
      # entire file to STDOUT, then delete it.  Drawbacks: download doesn't begin
      # until zip file creation is complete.  Performance mitigations: we create
      # the zip file using Z_NO_COMPRESSION for fastest creation and decompress
      # speed (the theory being that mp3 files are already optimally compressed,
      # thus we aren't using zip format to obtain compression, but rather to
      # package multiple dir-organized files with integrity checking into a
      # single downloadable file), and write it to SSD (so pretty fast).
      # Benefits: having the complete zip file in hand prior to download allows
      # us to supply content_length in the response header so the http client can
      # see a finite-sized download progressing. $absZipfnm (along with all other
      # files in $td) is auto-deleted when this process exits (the download
      # having completed by then), preventing runaway tempfile disk consumption.
      #
      # The alternative approach would be to stream the zip file bytes as
      # they're being created onto the wire.  Benefits: possibly shorter time
      # before download commences, no concerns about free disk space
      # availability, time consuming disk writes, or temp file cleanup.  But
      # since most albums are no more than a few hundred MB, it doesn't take much
      # time to write that many bytes, tempfile auto-cleanup is a solved problem,
      # and time-to-first zipfile byte being downloaded is often gated by the
      # time required to spinup the HDD containing the mp3 files, a delay which
      # affects both approaches similarly (and dwarfs the time required to write
      # a few hundred MB to SSD).  And of course we can't supply content_length
      # in the response header since it isn't known yet.  In
      # https://www.perlmonks.org/?node_id=929668 an author of zip module gives
      # streaming examples, BUT! these streaming examples rely on a feature
      # FilterEnvelope nee FilterContainer that (since 2011) NEVER made it into
      # the released version (or alternatively IS NOT DOCUMENTED)!!!  Maybe
      # there are clues as to how the same end can be accomplished by other means
      # (rendering these Filter... options unnecessary)?

      my $filterNamer = sub { # this could be (and was) far more terse, but these comments might save future pain...
         # https://perldoc.perl.org/IO::Compress::Zip#File-Naming-Options
         # On entry to the sub the $_ variable will contain the name to be
         # filtered.  If no filename is available $_ will contain an empty
         # string.  The value of $_ when the sub returns will be used as the
         # archive member name.
         $_ = $tempFnmMap{$_} // $_;  # explicitly modify $_
         s!$reFsNmAbs!!;              # implicitly modify $_  NB: syntax differs slightly from map BLOCK used to gen @mappedSrcFnms
         # NB: next line does NOTHING, but doesn't fail either; apparently this closure/sub when called does not have access to $dl?
         # $dl->( "  FilterName: $_" );
         };
      my $zipOk = zip \@srcfnms => $absZipfnm
                     , Level => Z_NO_COMPRESSION
                     , Minimal => 1
                     , FilterName => $filterNamer
                     ;
      if( !$zipOk ) {
         $dl->( "${p}zip done: \n" );
         $dl->( $qy->ol( [ "Toverall: ".pr_et_us( time() - $tm_start ) ] ), "\n" );
         $dl->( $qy->end_html );
         die "zip failed: $ZipError\n";
         }

      # ok, $absZipfnm was created: send it to the client!
      print $qy->header( -type=>'application/zip', -attachment=>$zipfnm, -content_length=>(stat $absZipfnm)[7] );
      {
      open my $ifh, "<", $absZipfnm or die "can't open $absZipfnm for reading: $!\n";
      binmode $ifh;
      binmode STDOUT;
      my $bufsize = 4*1024*1024;
      local $INPUT_RECORD_SEPARATOR = \$bufsize;
      local $OUTPUT_AUTOFLUSH = 1;
      print $_ while( <$ifh> );
      }
         # my $locn = "$weblink/$fn" =~ s{([\s])}{sprintf("%%%02X",ord($1))}egr;  # poor man's urlencode (for '\s' only FTTB)

      last;
      }
   exit 0;
   }  # unimpl

sub search_keys_find_matches { my ($qy) = @_;
   my $isaset= $qy->Vars->{search_scope} // 'bavm'; # default to Book search
   my $skeys = $qy->Vars->{search_keys};
      # print(sprintf("%v04X", $skeys), "\n");
      # {
      # local $Data::Dumper::Useqq = 1;
      # print( decode_utf8(Dumper($skeys)) );
      # print( decode_utf8(encode_utf8($skeys)) );
      # }
      $skeys =~ s|&#9830;| |g; # remove nasty black-diamond-suit char that browser can inject when user hits browser-'back'.
      $skeys =~ s=${\q1}==g; # mimic qf file rename behavior
   my @search_keys = split( qr{[-\s,.:${\q2}]+}, $skeys ); # preserve: [+#]
   @search_keys = grep { not m{^(?:and|[Bb]y)$} } @search_keys;  # drop common paste noise terms
   my $norm_search_terms = join(' ',@search_keys);
   my $pat = gen_re_match_all_anyorder( '\w_', @search_keys );

   # WIP to allow processes colliding on flock $semfh to try the set of sem files in random/shuffle order
   # this would also require separating find-phase from search phase as the following should seek to complete all find-phases before searching
   #
   # my @tlixs;
   # for my $ix (0 .. $#treelocns) {
   #    my ($isa,$fsroot,$webroot,$findtype) = @{$treelocns[$ix]}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
   #    next unless $isa =~ m=[$isaset]=;
   #    push @tlixs, $ix;
   #    }
   # @tlixs = shuffle(@tlixs);  requires  -->  use List::Util 'shuffle';
   # while( scalar @tlixs ) {
   #    for my $ix (0 .. $#tlixs) {
   #       for my $hr ( @tlixs ) {
   #          my ($isa,$fsroot,$webroot,$findtype) = @{$hr}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
   #          my $matches = search_treelocn($qy,$isa,$fsroot,$webroot,$findtype,$pat);
   #          if( $matches ) {
   #             $hr->{matches} = $matches;
   #
   #             }
   #          }
   #       }
   #    }

   my $su = $qy->self_url =~ s=\?.*==r;
   # print $su;
   my ($cands,$matches) = (0, 0);  # counters accumulated across all calls to search_treelocn
   for my $hr ( @treelocns ) {
      my ($isa,$fsroot,$webroot,$findtype) = @{$hr}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
      next unless $isa =~ m=[$isaset]=;
      my ($ccnt, $mcnt); ($hr->{matches}, $ccnt, $mcnt) = search_treelocn($qy,$isa,$fsroot,$webroot,$findtype,$pat,$su);
      $cands   += $ccnt;
      $matches += $mcnt;
      }
   return ($norm_search_terms,$pat,$cands,$matches);
   }

sub handler_search_keys { my ($qy) = @_;
   my ($norm_search_terms,$pat,$cands,$matches) = search_keys_find_matches( $qy );
   print $qy->header;
   print $qy->start_html(
         -title=>$norm_search_terms,
         -base=>'true', -target=>'_blank',  # links on this page will open in new client (browser) tab/window.
         );
   print "$pat\n" if showRegex;
   # print $qy->Dump();
   print $qy->h3("Found",($matches||"no"),"matches of '$norm_search_terms' among",$cands,"candidates",$qy->a({href=>"/"},"new search")), "\n";
   for my $hr ( @treelocns ) {
      my $ms = $hr->{matches};
      if( scalar keys %{$ms} ) {
         my $cat = $hr->{cat};
         print $qy->a({href=>"#$cat"},$cat),"\n";
         }
      }
   for my $hr ( @treelocns ) {
      my $ms = $hr->{matches};
      if( scalar keys %{$ms} ) {
         my $anch = { id => $hr->{cat} };
         for my $yr (sort { $b cmp $a } keys %{$ms}) { # print "$yr:\n";
            print $qy->h3( $anch, $hr->{cat} . " &copy;".($yr || "<i>unknown</i>") ), "\n";
            $anch = {};
            print $qy->ol( map { $qy->li( $_ ); } sort @{$ms->{$yr}} ), "\n";
            }
         }
      }
   print $qy->h3( "Server response timing:" );
   print $qy->ol( [ "Toverall: ".pr_et_us( time() - $tm_start ), " Tfind: ".pr_et_us( $et_find ), " Tsrch: ".pr_et_us( $et_srch ) ] ), "\n";
   print $qy->end_html;
   exit 0;
   }
