#!/usr/bin/perl -T
use strict;
use warnings;
use Time::HiRes qw( time );
use Encode;
use Data::Dumper;
use Fcntl qw(:flock SEEK_END);

# see STDERR -> /var/log/nginx/error.log for errors executing this script (ubuntu-specific?)
# testing cmdlines using http://perldoc.perl.org/CGI.html#DEBUGGING
# time ./search-files search_scope=b search_keys='nginx EPUB'
#
# use Cwd;
# my $pwd = getcwd;
# my $username = getpwuid($<);
# print STDERR "\npwd=$pwd\nusername=$username\nHOME=$ENV{HOME}\n";

use constant cachedir => "/cgi-app-cache/search-files";  # ugly hack: these two dirs are OWNED and writable only by www-data
use constant common_fsroot => '/mnt/smb/5t_a/data';

# mode controls:
use constant showRegex => 0;
use constant count_uniq_cands   => 0;  # has major performance impact
use constant count_uniq_matches => 1;  # has no    performance impact

$ENV{PATH} = '/bin:/usr/bin';  # to allow running under 'taint mode' (-T in shebang)

my ($tm_start,$et_srch,$et_find) = (time(), 0, 0);

sub mtime { -f $_[0] ? (stat($_[0]))[9] : 0 ; }

use constant maxCopyrightYr => do {
   my ($sec,$min,$hour,$mday,$mon,$year,$wday,$yday,$isdst) = localtime();
   $year = $year+1900;  # std xlation
   $year + 1;  # (c)Y+1 books appear late in year Y
   };

use constant q1 => "'";
use constant q2 => '"';
use constant bound_re        => '(?:\b|[_])' ;  # https://dev.to/kirklewis/string-interpolation-of-constants-in-perl-5-181o
use constant bound_re_before => '(?:^|[\W_])';  # can't use \b because $qm may not consist of word chars only
use constant bound_re_after  => '(?:$|[\W_])';  # can't use \b because $qm may not consist of word chars only

sub gen_re_match_all_anyorder {
   my %tok_remap = ( and => '(?:N|AND)', 'c#' => 'CSHARP', 'f#' => 'FSHARP' );
   my @reraw;
   for (@_) {  # construct regex matching lines containing, in any order, ALL of @_
      my $qm = quotemeta( $_ );            #  match fragment
      $qm = $tok_remap{lc $qm} if exists $tok_remap{lc $qm};
      $qm = "${\bound_re_before}$qm${\bound_re_after}" if ($qm =~ m=[A-Z]=) && ! ($qm =~ m=[a-z]=); # all caps? match word
      push @reraw, "(?=.*$qm)"; # https://stackoverflow.com/questions/4389644/regex-to-match-string-containing-two-names-in-any-order
      }
   my $rv = '^(?i)' . join('',@reraw) . '.*$'; # print "\npat=$rv\n";
   return qr/$rv/;
   }

sub extract_yr { my($tgt) = @_;
   my @yr4s = $tgt =~ m=${\bound_re}(\d{4})${\bound_re}=g; # normally, (c) year is given as yyyy
   my @more = $tgt =~ m=${\bound_re}(\d{4})\d{2}${\bound_re}=g;   push( @yr4s, @more     ); # but rarely I give yyyymm
      @more = $tgt =~ m=${\bound_re}(\d{4})\d{4}${\bound_re}=g;   push( @yr4s, @more     ); # and rarely I give yyyymmdd
   my ($yr2)= $tgt =~ m=${\bound_re}(\d{2})\.\d{2}${\bound_re}=g; push( @yr4s, $yr2+2000 ) if $yr2; # more rarely I give yy.mm
   my $rv = '';
   if( @yr4s ) {
      my $max = 0;
      $max = ($_<=maxCopyrightYr && $_>$max) ? $_ : $max foreach @yr4s;  # print $max,"\n";
      $rv = $max if $max > 0;
      }
   $rv;
   }

sub search_treelocn { my ($qy,$isa,$fsroot,$webroot,$findtype,$pat,$su) = @_;
   # print STDERR "fsroot=$fsroot\n";
   my ($fsrt_tail) = $fsroot =~ m|([^/]+)$|;
   # print STDERR "fsrt_tail=$fsrt_tail\n";
   my $findoutfnm = cachedir."/$fsrt_tail";
   {  # $semfh is used in case fcgiwrap does not single-thread CGI processes;
      # this is not tremendously efficient in case of collision, but not awful either
      # _could_ fail flock immediately (LOCK_NB) and try a different $fsroot to parallel-process find-checking (and worst-case: scanning)
      my $semfh;
      if( 1 ) {
         my $semfnm = cachedir."/$fsrt_tail.sem";
         open $semfh, '>', $semfnm or die "abend: cannot open $semfnm for writing: $!\n";
         flock( $semfh, LOCK_EX )  or die "abend: flock $semfnm failed: $!\n";
         }
      my $modlogfnm = "$fsroot/.modify.log";
      my ($mtime_modlog , $mtime_findout) = (mtime($modlogfnm),mtime($findoutfnm));
      if( $mtime_modlog < $mtime_findout ) {
         # print STDERR "HIT! $findoutfnm newer than $modlogfnm\n";
         }
      else {
         my $findcmd = "cd $fsroot && find . -type $findtype > $findoutfnm";
         # print STDERR "running '$findcmd'\n";
         my $t_find_start = time();
         system( $findcmd );
         $et_find += time() - $t_find_start;
         }
      close $semfh if $semfh;
   }
   my ($cands, $matches) = (0, 0);
   my (%unique_cands,%unique_matches);
   my (%rv,%ft_f_keys,%extsOf);
   my $split_fnm_base_ext = ($findtype eq 'd')
      ? sub { ($_[0],''); }   # note that WE USE AN EXTENDED DEFINITION OF "ext" as defined HERE_EXT
      : sub { ($_[0] =~ m=(.+?)((?:[_.][Cc]ode|\.medtype|_cropped)?\.[^\.]+)$=); } ;  # <-- HERE_EXT
   my $t_srch_start = time();
   {
   open my $ifh, '<', $findoutfnm or die "abend: cannot open $findoutfnm for reading: $!\n";
   while ( my $fsNmRel = <$ifh> ) {  # one filename or dirname (relative to $fsroot) per line
      chomp $fsNmRel;
      my ($fnm) = $fsNmRel =~ m=([^/]+)$=; # print "$fnm\n";
      my $tgt = ".$fnm.";
      my ($base,$ext);
      if( count_uniq_cands ) {
         ($base,$ext) = $split_fnm_base_ext->($fnm);
         ++$unique_cands{$base};
         }
      ++$cands;
      if( $tgt =~ m,$pat, ) { # print "$tgt\n" ;
         ($base,$ext) = $split_fnm_base_ext->($fnm) unless $base;
         ++$unique_matches{$base} if count_uniq_matches;
         ++$matches;
         my $yr = extract_yr( $tgt );
         my $fsNmAbs = "$fsroot/$fsNmRel" =~ s=/\./=/=r; # print "$fsNmAbs\n" ;
         my $weblink = $fsNmAbs =~ s=^$fsroot=$webroot=r; # print "weblink=$weblink\n";
         if( $findtype eq 'd' || ! $ext ) {
            my $reldir = $weblink =~ s=^$webroot/==r;
            push @{$rv{$yr}}, $qy->a({href=>"$weblink"},$reldir) .' '. $qy->a({href=>"$su?zipdowndirnm=$weblink"},'zipdown!');
            }
         else {
            my $link_wo_ext = substr( $weblink, 0, - length $ext );
            $ft_f_keys{$link_wo_ext} = $yr;
            push @{$extsOf{$link_wo_ext}}, $ext;
            }
         }
      }
   } # closes $ifh
   for my $link_wo_ext (sort keys %ft_f_keys) {  # fold extsOf into rv
      my @out = $link_wo_ext =~ m=([^/]+)$=;
      my $lwx = $link_wo_ext =~ s{([#])}{sprintf("%%%02X",ord($1))}egr;  # poor man's urlencode (for '#' only FTTB)
      for my $ext ( reverse sort @{$extsOf{$link_wo_ext}} ) {  # reverse sort is a marginal first approximation of the 'order of [type] preference' described above
         push @out, $qy->a({href=>"$lwx$ext"},$ext),"\n";
         }
      my $yr = $ft_f_keys{$link_wo_ext};
      push @{$rv{$yr}}, join(' ',@out);
      }
   $et_srch += time() - $t_srch_start;
   return (\%rv,
            scalar keys %unique_cands   || $cands  ,
            scalar keys %unique_matches || $matches,
          );
   }

#
# As noted in README.md
#    * suffix variants: `2005.ext` might morph to either
#        * `2005.medtype.ext` generated by [Calibre](https://calibre-ebook.com/ ) when creating PDF from non-PDF, or
#        * `2005_cropped.ext` generated by [briss2](https://github.com/fwmechanic/briss2 ) when cropping a PDF.
# the vast majority of the time, a user will want to download the (cropped) pdf
# version of the ebook.  Since there are potentially many variants (formats and
# croppings) of the SAME book (in INCREASING order of preference:
#    * book.(chm|djvu|azw3|mobi|epub)
#    * book.azw3
#    * book.mobi
#    * book.epub
#    * book.pdf
#    * book_cropped.pdf  (locally generated from book.pdf)
#    * book.medtype.pdf  (locally generated from book.(chm|azw3|mobi|epub))
# ) it would be helpful for these to collapse into a single line containing
# multiple links, with the best version ( being left-most (and the longest
# link).
#
# What I think this means is I want to determine the basename (book) and find
# all files having the same basename and compress them as above.
#
my @treelocns = (  # unfortunately necessary hardcoding of app filesys/webapp mappings
   # HACK ALERT: handler_search_keys call tree adds {matches} to these array entries!!!
   # align with /etc/nginx/sites-enabled/default (must edit as root)
   { isa=>'b', ft=>'f', cat=>'Books'      , webroot=>'/files/ebooks'          , fsroot=>common_fsroot.'/ebooks'     },
   { isa=>'m', ft=>'d', cat=>'Music'      , webroot=>'/files/mp3'             , fsroot=>common_fsroot.'/MP3'        },
   { isa=>'a', ft=>'f', cat=>'Audiobooks' , webroot=>'/files/audiobooks'      , fsroot=>common_fsroot.'/audiobooks' },
   { isa=>'v', ft=>'d', cat=>'Videos'     , webroot=>'/files/video-downloads' , fsroot=>common_fsroot.'/Video'      },
   );

use CGI;
my $qy = CGI->new;
# dispatch
sub rsp404 {
   print $qy->header( -status => '404 Not Found', );
   exit 0;
   }
if   ( exists $qy->Vars->{search_keys}  ) { handler_search_keys( $qy ); }
elsif( exists $qy->Vars->{zipdowndirnm} ) { handler_zipdownload( $qy ); }
else { rsp404(); }

sub handler_zipdownload { my ($qy) = @_;
   my $weblink = $qy->Vars->{zipdowndirnm};  print "weblink=$weblink<p>\n";
   print $qy->header;
   print $qy->start_html(
         -title=>"zipdownload $weblink",
         -base=>'true', -target=>'_blank',  # links on this page will open in new client (browser) tab/window.
         );
   for my $hr ( @treelocns ) {
      my ($fsroot,$webroot,$findtype) = @{$hr}{qw(fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
      next unless $weblink =~ m=^\Q$webroot=;
      print "hit on webroot=$webroot";
      print " but findtype=$findtype != 'd'" unless $findtype eq 'd';
      print "<p>\n";
      my $fsNmAbs = $weblink =~ s=^\Q$webroot=$fsroot=r;
      print "fsNmAbs=$fsNmAbs";
      print " but it isn't a directory!" unless -d $fsNmAbs;
      print "<p>\n";
      }

   # die unless $FsNmAbs dir tree size is < 1GB (or something more sophisticated)
   # do the needful to request mod_zip do its thing...
   #
   # Other items
   # handler_search_keys: when generating dir results, generate additional link
   #   (icon?) "download tree-zip" containing link to this page with some param
   #   that routes to handler_zipdownload.
   #
   # outside repo: modify nginx config to route

   print $qy->end_html;
   exit 0;
   }  # unimpl

sub search_keys_find_matches { my ($qy) = @_;
   my $isaset= $qy->Vars->{search_scope} // 'bavm'; # default to Book search
   my $skeys = $qy->Vars->{search_keys};
      # print(sprintf("%v04X", $skeys), "\n");
      # {
      # local $Data::Dumper::Useqq = 1;
      # print( decode_utf8(Dumper($skeys)) );
      # print( decode_utf8(encode_utf8($skeys)) );
      # }
      $skeys =~ s|&#9830;| |g; # remove nasty black-diamond-suit char that browser can inject when user hits browser-'back'.
      $skeys =~ s=${\q1}==g; # mimic qf file rename behavior
   my @search_keys = split( qr{[-\s,.:${\q2}]+}, $skeys ); # preserve: [+#]
   my $norm_search_terms = join(' ',@search_keys);
   my $pat = gen_re_match_all_anyorder( @search_keys );

   # WIP to allow processes colliding on flock $semfh to try the set of sem files in random/shuffle order
   # this would also require separating find-phase from search phase as the following should seek to complete all find-phases before searching
   #
   # my @tlixs;
   # for my $ix (0 .. $#treelocns) {
   #    my ($isa,$fsroot,$webroot,$findtype) = @{$treelocns[$ix]}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
   #    next unless $isa =~ m=[$isaset]=;
   #    push @tlixs, $ix;
   #    }
   # @tlixs = shuffle(@tlixs);  requires  -->  use List::Util 'shuffle';
   # while( scalar @tlixs ) {
   #    for my $ix (0 .. $#tlixs) {
   #       for my $hr ( @tlixs ) {
   #          my ($isa,$fsroot,$webroot,$findtype) = @{$hr}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
   #          my $matches = search_treelocn($qy,$isa,$fsroot,$webroot,$findtype,$pat);
   #          if( $matches ) {
   #             $hr->{matches} = $matches;
   #
   #             }
   #          }
   #       }
   #    }

   my $su = $qy->self_url =~ s=\?.*==r;
   # print $su;
   my ($cands,$matches) = (0, 0);  # counters accumulated across all calls to search_treelocn
   for my $hr ( @treelocns ) {
      my ($isa,$fsroot,$webroot,$findtype) = @{$hr}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
      next unless $isa =~ m=[$isaset]=;
      my ($ccnt, $mcnt); ($hr->{matches}, $ccnt, $mcnt) = search_treelocn($qy,$isa,$fsroot,$webroot,$findtype,$pat,$su);
      $cands   += $ccnt;
      $matches += $mcnt;
      }
   return ($norm_search_terms,$pat,$cands,$matches);
   }

sub handler_search_keys { my ($qy) = @_;
   my ($norm_search_terms,$pat,$cands,$matches) = search_keys_find_matches( $qy );
   print $qy->header;
   print $qy->start_html(
         -title=>$norm_search_terms,
         -base=>'true', -target=>'_blank',  # links on this page will open in new client (browser) tab/window.
         );
   print "$pat\n" if showRegex;
   # print $qy->Dump();
   print $qy->h3("Found",($matches||"no"),"matches of '$norm_search_terms' among",$cands,"candidates",$qy->a({href=>"/"},"new search")), "\n";
   for my $hr ( @treelocns ) {
      my $ms = $hr->{matches};
      if( scalar keys %{$ms} ) {
         my $cat = $hr->{cat};
         print $qy->a({href=>"#$cat"},$cat),"\n";
         }
      }
   for my $hr ( @treelocns ) {
      my $ms = $hr->{matches};
      if( scalar keys %{$ms} ) {
         my $anch = { id => $hr->{cat} };
         for my $yr (reverse sort keys %{$ms}) { # print "$yr:\n";
            print $qy->h3( $anch, $hr->{cat} . " &copy;".($yr || "<i>unknown</i>") ), "\n";
            $anch = {};
            print $qy->ol( map { $qy->li( $_ ); } sort @{$ms->{$yr}} ), "\n";
            }
         }
      }
   sub pr_et_us { return sprintf("%.6f", $_[0]); }
   print $qy->h3( "Server response timing:" );
   print $qy->ol( [ "Toverall: ".pr_et_us( time() - $tm_start ), " Tfind: ".pr_et_us( $et_find ), " Tsrch: ".pr_et_us( $et_srch ) ] ), "\n";
   print $qy->end_html;
   exit 0;
   }
