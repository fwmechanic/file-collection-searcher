#!/usr/bin/perl -T
# -T == taint mode: https://perldoc.perl.org/perlsec#Laundering-and-Detecting-Tainted-Data

use strict;
use warnings;
use Time::HiRes qw( time );
# use Encode;
use English;
use Data::Dumper;
use Fcntl qw(:flock SEEK_END);
# ^^^-end core Perl modules, vvv-begin non-core modules:
use CGI;  # from `apt-get install libcgi-pm-perl`
use CGI::Carp qw(fatalsToBrowser warningsToBrowser set_message);  # from `apt-get install libcgi-pm-perl`

use FindBin qw( $RealBin );
my $untainted_bin;  # Taint Mode gyrations courtesy of https://www.perlmonks.org/?node_id=11120715 (last answer)
BEGIN { ($untainted_bin) = $RealBin =~ /(.+)/; }
use lib "$untainted_bin";
use Site;

# see STDERR -> /var/log/nginx/error.log for errors executing this script (ubuntu-specific?)
# testing cmdlines using http://perldoc.perl.org/CGI.html#DEBUGGING
# time ./search-files search_scope=b search_keys='nginx EPUB'
#
# use Cwd;
# my $pwd = getcwd;
# my $username = getpwuid($<);
# print STDERR "\npwd=$pwd\nusername=$username\nHOME=$ENV{HOME}\n";

# mode controls:
use constant showRegex => 0;

$ENV{PATH} = '/bin:/usr/bin';  # to allow running under 'taint mode' (-T in shebang)

my ($tm_start,$et_srch,$et_find) = (time(), 0, 0);

use constant q1 => "'";
use constant q2 => '"';
use constant bound_re => '(?:\b|[_])' ;  # https://dev.to/kirklewis/string-interpolation-of-constants-in-perl-5-181o

sub gen_re_match_all_anyorder {
   # https://www.perlmonks.org/?node_id=308753 (thread: https://www.perlmonks.org/?node_id=308744)
   # to make a specialized version of \b that views "-" and "/" as "word characters" (sort of), you might use something like this:
   # my $w = '\w/-';
   my $w = shift or die "gen_re_match_all_anyorder: no args?";
   my $b = "(?:(*negative_lookbehind:[$w])(*positive_lookahead:[$w])|(*positive_lookbehind:[$w])(*negative_lookahead:[$w]))";
   # my @words = ($rec =~ /${b}[$w]+${b}/g);

   # The following implements a brute-force solution to the performance problem
   # caused by use of \b (or $b, the specialized version of \b) to implement
   # whole-word search terms:
   #
   # The returned search regex consists of a sequence of look-ahead-assertions
   # (LAA), each matching one user search term.  Previously, a word search term
   # would be wrapped in \b's before being inserted in the (one and only) LAA
   # sequence in input (i.e. user-provided) order, however the presence of \b
   # (caused by the presence of ANY word search term) caused a huge (100% == 2x)
   # performance hit.
   #
   # This is resolved by adding _TWO_ LAA's into the returned LAA sequence for
   # each input word search term:
   # 1. a non-word-search (string) LAA is added to @strs for every search term.
   # 2. a word-search (\b) LAA is added to @words for every word search term.
   #
   # The returned sequence of LAA's is the concatenation of @strs followed by
   # @words.  Thus all candidate strings must FIRST pass all @strs LAA's; these
   # are very fast checks.  Only those few candidate strings passing all @strs
   # LAA's undergo checking against the (SLOW) @words LAA's.  Assuming only a low
   # percentage of candidate strings pass all @strs LAA's, the performance impact
   # of the trailing (SLOW) @words LAA's is reduced to almost nothing (and
   # testing shows this to be true).
   #
   # Given that our search is optimized by failing each candidate string as
   # quickly as possible, a further optimization is to sort @strs LAA's by
   # descending length: this causes the longest strings among the input search
   # terms to be searched for first.  The naive idea being that longer strings
   # are less likey to be found than short strings.
   #
   my (@strs,@words);
   for my $rawterm (@_) {  # construct regex matching lines containing, in any order, ALL of @_
      $rawterm =~ s|([CF])#|\1SHARP|;
      $rawterm =~ s|([cf])#|\1sharp|;
      my $term = quotemeta( $rawterm );  #  match term
      push @strs,  "(*positive_lookahead:.*$term)"; # https://stackoverflow.com/a/4389683 https://stackoverflow.com/questions/4389644/regex-to-match-string-containing-two-names-in-any-order
      push @words, "(*positive_lookahead:.*$b$term$b)" if ($term =~ m=[A-Z]=) && ($term !~ m=[a-z]=); # all term alphas are caps (i.e. at least one uppercase-alpha and no lowercase-alphas)?: match word as defined by $b + $w
      }
   @strs = sort { length $b <=> length $a } @strs;  # try to find longest strings first
   my $rv = '^(?i)' . join('', @strs) . join('', @words) . '.*$'; # print "\npat=$rv\n";
   return qr($rv);
   }

#------- (here..search_treelocn] to be moved to separate script

use constant maxCopyrightYr => do {
   my ($sec,$min,$hour,$mday,$mon,$year,$wday,$yday,$isdst) = localtime();
   $year = $year+1900;  # std xlation
   $year + 1;  # (c)Y+1 books appear late in year Y
   };

sub extract_yr { my($tgt) = @_;
   my @yr4s = $tgt =~ m=${\bound_re}(\d{4})${\bound_re}=g; # normally, (c) year is given as yyyy
   my @more = $tgt =~ m=${\bound_re}(\d{4})\d{2}${\bound_re}=g;   push( @yr4s, @more     ); # but rarely I give yyyymm
      @more = $tgt =~ m=${\bound_re}(\d{4})\d{4}${\bound_re}=g;   push( @yr4s, @more     ); # and rarely I give yyyymmdd
   my ($yr2)= $tgt =~ m=${\bound_re}(\d{2})\.\d{2}${\bound_re}=g; push( @yr4s, $yr2+2000 ) if $yr2; # more rarely I give yy.mm
   my $rv = '';
   if( @yr4s ) {
      my $max = 0;
      $max = ($_<=maxCopyrightYr && $_>$max) ? $_ : $max foreach @yr4s;  # print $max,"\n";
      $rv = $max if $max > 0;
      }
   $rv;
   }

sub a_href { my ($href,$user) = @_; return '<a href="' . $href . '">' . $user . '</a>'; }

sub mtime { -f $_[0] ? (stat($_[0]))[9] : 0 ; }

sub search_treelocn { my ($cachedir,$fsroot,$webroot,$findtype,$pat) = @_;
   # print STDERR "fsroot=$fsroot\n";
   my ($fsrt_tail) = $fsroot =~ m|([^/]+)$|;
   # print STDERR "fsrt_tail=$fsrt_tail\n";
   my $findoutfnm = $cachedir."/$fsrt_tail";
   {  # $semfh is used in case fcgiwrap does not single-thread CGI processes;
      # this is not tremendously efficient in case of collision, but not awful either
      # _could_ fail flock immediately (LOCK_NB) and try a different $fsroot to parallel-process find-checking (and worst-case: scanning)
      my $semfh;
      if( 1 ) {
         my $semfnm = $cachedir."/$fsrt_tail.sem";
         open $semfh, '>', $semfnm or die "abend: cannot open $semfnm for writing: $!\n";
         flock( $semfh, LOCK_EX )  or die "abend: flock $semfnm failed: $!\n";
         }
      my $modlogfnm = "$fsroot/.modify.log";
      my ($mtime_modlog , $mtime_findout) = (mtime($modlogfnm),mtime($findoutfnm));
      if( $mtime_modlog < $mtime_findout ) {
         # print STDERR "HIT! $findoutfnm newer than $modlogfnm\n";
         }
      else {
         my $ft = $findtype; $ft .= ',l' if $ft =~ m|f|;
         my $findcmd = "cd '$fsroot' && find . -type '$ft' > '$findoutfnm'";
         # print STDERR "running '$findcmd'\n";
         my $t_find_start = time();
         system( $findcmd );
         $et_find += time() - $t_find_start;
         }
      close $semfh if $semfh;
   }
   my $cands = 0;
   my (%unique_matches,%rv,%ft_f_keys,%extsOf);
   my $split_fnm_base_ext = ($findtype eq 'd')
      ? sub { ($_[0],''); }   # note that WE USE AN EXTENDED DEFINITION OF "ext" as defined HERE_EXT
      : sub { ($_[0] =~ m=(.+?)((?:[_.][Cc]ode|\.medtype|_cropped)?\.[^\.]+)$=); } ;  # <-- HERE_EXT
   my $t_srch_start = time();
   {
   open my $ifh, '<', $findoutfnm or die "abend: cannot open $findoutfnm for reading: $!\n";
   while ( my $fsNmRel = <$ifh> ) {  # one filename or dirname (relative to $fsroot) per line
      ++$cands;
      chomp $fsNmRel;
      my ($fnm) = $fsNmRel =~ m=([^/]+)$=; # print "$fnm\n";
      my $tgt = '.' . $fnm . '.';  # setup for match (next)
      if( $tgt =~ m,$pat, ) { # print "$tgt\n" ;  # MATCH!  slow path
         my ($base,$ext) = $split_fnm_base_ext->($fnm);
         ++$unique_matches{$base};
         my $yr = extract_yr( $tgt );
         my $fsNmAbs = "$fsroot/$fsNmRel" =~ s=/\./=/=r; # print "$fsNmAbs\n" ;
         my $weblink = $fsNmAbs =~ s=^$fsroot=$webroot=r; # print "weblink=$weblink\n";
         if( $findtype eq 'd' || ! $ext ) {
            my $reldir = $weblink =~ s=^$webroot/==r;
            push @{$rv{$yr}}, a_href( $weblink, $reldir ) .' '. a_href( "zipdown?zipdowndirnm=$weblink", 'zipdown!' );
            }
         else {
            my $link_wo_ext = substr( $weblink, 0, - length $ext );
            $ft_f_keys{$link_wo_ext} = $yr;
            push @{$extsOf{$link_wo_ext}}, $ext;
            }
         }
      }
   } # closes $ifh
   for my $link_wo_ext (sort keys %ft_f_keys) {  # fold extsOf into rv
      my @out = $link_wo_ext =~ m=([^/]+)$=;
      my $lwx = $link_wo_ext =~ s{([#])}{sprintf("%%%02X",ord($1))}egr;  # poor man's urlencode (for '#' only FTTB)
      for my $ext ( sort { $b cmp $a } @{$extsOf{$link_wo_ext}} ) {  # reverse sort is a marginal first approximation of the 'order of [type] preference' described above
         push @out, a_href( "$lwx$ext", $ext ) . "\n";
         }
      my $yr = $ft_f_keys{$link_wo_ext};
      push @{$rv{$yr}}, join(' ',@out);
      }
   $et_srch += time() - $t_srch_start;
   return ( \%rv
          , $cands
          , scalar keys %unique_matches,
          );
   }

handler_search_keys( CGI->new );

sub search_keys_find_matches { my ($qy) = @_;
   my $isaset= $qy->Vars->{search_scope} // 'bavm'; # default to Book search
   my $skeys = $qy->Vars->{search_keys};
      # print(sprintf("%v04X", $skeys), "\n");
      # {
      # local $Data::Dumper::Useqq = 1;
      # print( decode_utf8(Dumper($skeys)) );
      # print( decode_utf8(encode_utf8($skeys)) );
      # }
      $skeys =~ s|&#9830;| |g; # remove nasty black-diamond-suit char that browser can inject when user hits browser-'back'.
      $skeys =~ s=${\q1}==g; # mimic qf file rename behavior
   my @search_keys = split( qr{[-\s,.:${\q2}]+}, $skeys ); # preserve: [+#]
   @search_keys = grep { not m{^(?:and|[Bb]y)$} } @search_keys;  # drop common paste noise terms
   my $norm_search_terms = join(' ',@search_keys);
   my $pat = gen_re_match_all_anyorder( '\w_', @search_keys );
   my ($cands,$matches) = (0, 0);  # counters accumulated across all calls to search_treelocn
   for my $hr ( @Site::treelocns ) {
      my ($isa,$fsroot,$webroot,$findtype) = @{$hr}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
      next unless $isa =~ m=[$isaset]=;
      my ($ccnt, $mcnt); ($hr->{matches}, $ccnt, $mcnt) = search_treelocn(Site::CACHEDIR,$fsroot,$webroot,$findtype,$pat);
      $cands   += $ccnt;
      $matches += $mcnt;
      }
   return ($norm_search_terms,$pat,$cands,$matches);
   }

sub pr_et_us { return sprintf("%.6f", $_[0]); }

sub handler_search_keys { my ($qy) = @_;
   exists $qy->Vars->{search_keys} or die "missing 'search_keys' param\n?";
   my ($norm_search_terms,$pat,$cands,$matches) = search_keys_find_matches( $qy );
   print $qy->header;
   print $qy->start_html(
         -title=>$norm_search_terms,
         -base=>'true', -target=>'_blank',  # links on this page will open in new client (browser) tab/window.
         );
   print "$pat\n" if showRegex;
   # print $qy->Dump();
   print $qy->h3("Found",($matches||"no"),"matches of '$norm_search_terms' among",$cands,"candidates",$qy->a({href=>"/"},"new search")), "\n";
   for my $hr ( @Site::treelocns ) {
      my $ms = $hr->{matches};
      if( scalar keys %{$ms} ) {
         my $cat = $hr->{cat};
         print $qy->a({href=>"#$cat"},$cat),"\n";
         }
      }
   for my $hr ( @Site::treelocns ) {
      my $ms = $hr->{matches};
      if( scalar keys %{$ms} ) {
         my $anch = { id => $hr->{cat} };
         for my $yr (sort { $b cmp $a } keys %{$ms}) { # print "$yr:\n";
            print $qy->h3( $anch, $hr->{cat} . " &copy;".($yr || "<i>unknown</i>") ), "\n";
            $anch = {};
            print $qy->ol( map { $qy->li( $_ ); } sort @{$ms->{$yr}} ), "\n";
            }
         }
      }
   print $qy->h3( "Server response timing:" );
   print $qy->ol( [ "Toverall: ".pr_et_us( time() - $tm_start ), " Tfind: ".pr_et_us( $et_find ), " Tsrch: ".pr_et_us( $et_srch ) ] ), "\n";
   print $qy->end_html;
   exit 0;
   }
