#!/usr/bin/perl -T
use strict;
use warnings;
use Time::HiRes qw( time );
use Encode;
use Data::Dumper;
use Fcntl qw(:flock SEEK_END);

# see STDERR -> /var/log/nginx/error.log for errors executing this script (ubuntu-specific?)
# testing cmdlines using http://perldoc.perl.org/CGI.html#DEBUGGING
# time ./search-files search_scope=b search_keys='nginx EPUB'
#
# use Cwd;
# my $pwd = getcwd;
# my $username = getpwuid($<);
# print STDERR "\npwd=$pwd\nusername=$username\nHOME=$ENV{HOME}\n";

use constant cachedir => "/cgi-app-cache/search-files";  # ugly hack: these two dirs are OWNED and writable only by www-data
use constant common_fsroot => '/var/www-filesearcher-data';  # contains symlinks to actual file trees

# mode controls:
use constant showRegex => 0;
use constant count_uniq_cands   => 0;  # has major performance impact
use constant count_uniq_matches => 1;  # has no    performance impact

$ENV{PATH} = '/bin:/usr/bin';  # to allow running under 'taint mode' (-T in shebang)

my ($tm_start,$et_srch,$et_find) = (time(), 0, 0);

sub mtime { -f $_[0] ? (stat($_[0]))[9] : 0 ; }

use constant maxCopyrightYr => do {
   my ($sec,$min,$hour,$mday,$mon,$year,$wday,$yday,$isdst) = localtime();
   $year = $year+1900;  # std xlation
   $year + 1;  # (c)Y+1 books appear late in year Y
   };

use constant q1 => "'";
use constant q2 => '"';
use constant bound_re        => '(?:\b|[_])' ;  # https://dev.to/kirklewis/string-interpolation-of-constants-in-perl-5-181o

sub gen_re_match_all_anyorder {
   # https://www.perlmonks.org/?node_id=308753 (thread: https://www.perlmonks.org/?node_id=308744)
   # to make a specialized version of \b that views "-" and "/" as "word characters" (sort of), you might use something like this:
   # my $w = '\w/-';
   my $w = shift or die "gen_re_match_all_anyorder: no args?";
   my $b = "(?:(*negative_lookbehind:[$w])(*positive_lookahead:[$w])|(*positive_lookbehind:[$w])(*negative_lookahead:[$w]))";
   # my @words = ($rec =~ /${b}[$w]+${b}/g);

   # The following implements a brute-force solution to the performance problem
   # caused by use of \b (or $b, the specialized version of \b) to implement
   # whole-word search terms:
   #
   # The returned search regex consists of a sequence of look-ahead-assertions
   # (LAA), each matching one user search term.  Previously, a word search term
   # would be wrapped in \b's before being inserted in the (one and only) LAA
   # sequence in input (i.e. user-provided) order, however the presence of \b
   # (caused by the presence of ANY word search term) caused a huge (100% == 2x)
   # performance hit.
   #
   # This is resolved by adding _TWO_ LAA's into the returned LAA sequence for
   # each input word search term:
   # 1. a non-word-search (string) LAA is added to @strs for every search term.
   # 2. a word-search (\b) LAA is added to @words for every word search term.
   #
   # The returned sequence of LAA's is the concatenation of @strs followed by
   # @words.  Thus all candidate strings must FIRST pass all @strs LAA's; these
   # are very fast checks.  Only those few candidate strings passing all @strs
   # LAA's undergo checking against the (SLOW) @words LAA's.  Assuming only a low
   # percentage of candidate strings pass all @strs LAA's, the performance impact
   # of the trailing (SLOW) @words LAA's is reduced to almost nothing (and
   # testing shows this to be true).
   #
   # Given that our search is optimized by failing each candidate string as
   # quickly as possible, a further optimization is to sort @strs LAA's by
   # descending length: this causes the longest strings among the input search
   # terms to be searched for first.  The naive idea being that longer strings
   # are less likey to be found than short strings.
   #
   my (@strs,@words);
   for my $rawterm (@_) {  # construct regex matching lines containing, in any order, ALL of @_
      my $term = quotemeta( $rawterm );  #  match term
      push @strs,  "(*positive_lookahead:.*$term)"; # https://stackoverflow.com/a/4389683 https://stackoverflow.com/questions/4389644/regex-to-match-string-containing-two-names-in-any-order
      push @words, "(*positive_lookahead:.*$b$term$b)" if ($term =~ m=[A-Z]=) && ($term !~ m=[a-z]=); # all term alphas are caps (i.e. at least one uppercase-alpha and no lowercase-alphas)?: match word as defined by $b + $w
      }
   @strs = sort { length $b <=> length $a } @strs;  # try to find longest strings first
   my $rv = '^(?i)' . join('', @strs) . join('', @words) . '.*$'; # print "\npat=$rv\n";
   return qr($rv);
   }

sub extract_yr { my($tgt) = @_;
   my @yr4s = $tgt =~ m=${\bound_re}(\d{4})${\bound_re}=g; # normally, (c) year is given as yyyy
   my @more = $tgt =~ m=${\bound_re}(\d{4})\d{2}${\bound_re}=g;   push( @yr4s, @more     ); # but rarely I give yyyymm
      @more = $tgt =~ m=${\bound_re}(\d{4})\d{4}${\bound_re}=g;   push( @yr4s, @more     ); # and rarely I give yyyymmdd
   my ($yr2)= $tgt =~ m=${\bound_re}(\d{2})\.\d{2}${\bound_re}=g; push( @yr4s, $yr2+2000 ) if $yr2; # more rarely I give yy.mm
   my $rv = '';
   if( @yr4s ) {
      my $max = 0;
      $max = ($_<=maxCopyrightYr && $_>$max) ? $_ : $max foreach @yr4s;  # print $max,"\n";
      $rv = $max if $max > 0;
      }
   $rv;
   }

sub search_treelocn { my ($qy,$isa,$fsroot,$webroot,$findtype,$pat,$su) = @_;
   # print STDERR "fsroot=$fsroot\n";
   my ($fsrt_tail) = $fsroot =~ m|([^/]+)$|;
   # print STDERR "fsrt_tail=$fsrt_tail\n";
   my $findoutfnm = cachedir."/$fsrt_tail";
   {  # $semfh is used in case fcgiwrap does not single-thread CGI processes;
      # this is not tremendously efficient in case of collision, but not awful either
      # _could_ fail flock immediately (LOCK_NB) and try a different $fsroot to parallel-process find-checking (and worst-case: scanning)
      my $semfh;
      if( 1 ) {
         my $semfnm = cachedir."/$fsrt_tail.sem";
         open $semfh, '>', $semfnm or die "abend: cannot open $semfnm for writing: $!\n";
         flock( $semfh, LOCK_EX )  or die "abend: flock $semfnm failed: $!\n";
         }
      my $modlogfnm = "$fsroot/.modify.log";
      my ($mtime_modlog , $mtime_findout) = (mtime($modlogfnm),mtime($findoutfnm));
      if( $mtime_modlog < $mtime_findout ) {
         # print STDERR "HIT! $findoutfnm newer than $modlogfnm\n";
         }
      else {
         my $findcmd = "cd $fsroot && find . -type $findtype > $findoutfnm";
         # print STDERR "running '$findcmd'\n";
         my $t_find_start = time();
         system( $findcmd );
         $et_find += time() - $t_find_start;
         }
      close $semfh if $semfh;
   }
   my ($cands, $matches) = (0, 0);
   my (%unique_cands,%unique_matches);
   my (%rv,%ft_f_keys,%extsOf);
   my $split_fnm_base_ext = ($findtype eq 'd')
      ? sub { ($_[0],''); }   # note that WE USE AN EXTENDED DEFINITION OF "ext" as defined HERE_EXT
      : sub { ($_[0] =~ m=(.+?)((?:[_.][Cc]ode|\.medtype|_cropped)?\.[^\.]+)$=); } ;  # <-- HERE_EXT
   my $t_srch_start = time();
   {
   open my $ifh, '<', $findoutfnm or die "abend: cannot open $findoutfnm for reading: $!\n";
   while ( my $fsNmRel = <$ifh> ) {  # one filename or dirname (relative to $fsroot) per line
      chomp $fsNmRel;
      my ($fnm) = $fsNmRel =~ m=([^/]+)$=; # print "$fnm\n";
      my $tgt = ".$fnm.";
      my ($base,$ext);
      if( count_uniq_cands ) {
         ($base,$ext) = $split_fnm_base_ext->($fnm);
         ++$unique_cands{$base};
         }
      ++$cands;
      if( $tgt =~ m,$pat, ) { # print "$tgt\n" ;
         ($base,$ext) = $split_fnm_base_ext->($fnm) unless $base;
         ++$unique_matches{$base} if count_uniq_matches;
         ++$matches;
         my $yr = extract_yr( $tgt );
         my $fsNmAbs = "$fsroot/$fsNmRel" =~ s=/\./=/=r; # print "$fsNmAbs\n" ;
         my $weblink = $fsNmAbs =~ s=^$fsroot=$webroot=r; # print "weblink=$weblink\n";
         if( $findtype eq 'd' || ! $ext ) {
            my $reldir = $weblink =~ s=^$webroot/==r;
            push @{$rv{$yr}}, $qy->a({href=>"$weblink"},$reldir) .' '. $qy->a({href=>"$su?zipdowndirnm=$weblink"},'zipdown!');
            }
         else {
            my $link_wo_ext = substr( $weblink, 0, - length $ext );
            $ft_f_keys{$link_wo_ext} = $yr;
            push @{$extsOf{$link_wo_ext}}, $ext;
            }
         }
      }
   } # closes $ifh
   for my $link_wo_ext (sort keys %ft_f_keys) {  # fold extsOf into rv
      my @out = $link_wo_ext =~ m=([^/]+)$=;
      my $lwx = $link_wo_ext =~ s{([#])}{sprintf("%%%02X",ord($1))}egr;  # poor man's urlencode (for '#' only FTTB)
      for my $ext ( reverse sort @{$extsOf{$link_wo_ext}} ) {  # reverse sort is a marginal first approximation of the 'order of [type] preference' described above
         push @out, $qy->a({href=>"$lwx$ext"},$ext),"\n";
         }
      my $yr = $ft_f_keys{$link_wo_ext};
      push @{$rv{$yr}}, join(' ',@out);
      }
   $et_srch += time() - $t_srch_start;
   return (\%rv,
            scalar keys %unique_cands   || $cands  ,
            scalar keys %unique_matches || $matches,
          );
   }

#
# As noted in README.md
#    * suffix variants: `2005.ext` might morph to either
#        * `2005.medtype.ext` generated by [Calibre](https://calibre-ebook.com/ ) when creating PDF from non-PDF, or
#        * `2005_cropped.ext` generated by [briss2](https://github.com/fwmechanic/briss2 ) when cropping a PDF.
# the vast majority of the time, a user will want to download the (cropped) pdf
# version of the ebook.  Since there are potentially many variants (formats and
# croppings) of the SAME book (in INCREASING order of preference:
#    * book.(djvu|azw3|chm|mobi|epub|pdf)
#    * book_cropped.pdf  (locally generated from book.pdf)
#    * book.medtype.pdf  (locally generated from book.(azw3|chm|mobi|epub))
#    * book.code.zip     (git repo(s) associated with book text)
# ) it would be helpful for these to collapse into a single line containing
# multiple links, with the best version ( being left-most (and the longest
# link).
#
# What I think this means is I want to determine the basename (book) and find
# all files having the same basename and compress them as above.
#
my @treelocns = (  # unfortunately necessary hardcoding of app filesys/webapp mappings
   # HACK ALERT: handler_search_keys call tree adds {matches} to these array entries!!!
   # align with /etc/nginx/sites-enabled/default (must edit as root)
   { isa=>'b', ft=>'f', cat=>'Books'      , webroot=>'/files/ebooks'       , fsroot=>common_fsroot.'/ebooks'       },
   { isa=>'m', ft=>'d', cat=>'Music'      , webroot=>'/files/MP3'          , fsroot=>common_fsroot.'/MP3'          },
 # { isa=>'m', ft=>'d', cat=>'Music'      , webroot=>'/files/MP3_extranea' , fsroot=>common_fsroot.'/MP3_extranea' },
   { isa=>'a', ft=>'f', cat=>'Audiobooks' , webroot=>'/files/audiobooks'   , fsroot=>common_fsroot.'/audiobooks'   },
   { isa=>'v', ft=>'d', cat=>'Videos'     , webroot=>'/files/Video'        , fsroot=>common_fsroot.'/Video'        },
   );

use CGI;
my $qy = CGI->new;
# dispatch
sub rsp404 {
   print $qy->header( -status => '404 Not Found', );
   exit 0;
   }
if   ( exists $qy->Vars->{search_keys}  ) { handler_search_keys( $qy ); }
elsif( exists $qy->Vars->{zipdowndirnm} ) { handler_zipdownload( $qy ); }
else { rsp404(); }

sub pr_et_us { return sprintf("%.6f", $_[0]); }

sub handler_zipdownload { my ($qy) = @_;
   my $p = '<br>';
   $qy->Vars->{zipdowndirnm} =~ m=^((?:(?!\.\./).)*)$=;   # untaint
   my $weblink = $1; print STDERR "weblink=$weblink$p\n";
   print $qy->header;
   print $qy->start_html(
         -title=>"zipdownload $weblink",
         -base=>'true', -target=>'_blank',  # links on this page will open in new client (browser) tab/window.
         );
   for my $hr ( @treelocns ) {
      my ($fsroot,$webroot,$findtype) = @{$hr}{qw(fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
      next unless $weblink =~ m=^\Q$webroot=;
      print $qy->h3("zipdownload $weblink"), "\n";
      print "hit on webroot=$webroot";
      print " but findtype=$findtype != 'd'" unless $findtype eq 'd';
      print "$p\n";
      my $fsNmAbs = $weblink =~ s=^\Q$webroot=$fsroot=r;
      print "fsNmAbs=$fsNmAbs";
      print " but it isn't a directory!" unless -d $fsNmAbs;
      print "$p\n";

      # https://www.nginx.com/resources/wiki/modules/zip/
      # response stream format
      # crc32 bytecount location filename\r\n
      # where
      #   crc32      of the file, or '-' if not available
      #   bytecount  of the file
      #   location   where the file can be read, in the context of the active nginx config file
      #              "(properly URL-encoded)" (which means any spaces in the location must be converted to e.g. '%20')
      #   filename   name the file written to the zipfile is given; may include a directory

      my $cmd = "cd '$fsNmAbs' && find . -type f -printf '%s %p\n'";
      print( $p, $cmd, "\n" );
      my @files = qx( cd '$fsNmAbs' && find . -type f -printf '%s %p\n' );
      print( $p, $_, "\n" ) for @files;
      my @mzrecs;
      for ( @files ) {  chomp;
         my ($bc,$fn) = split( ' ', $_, 2 );
         $fn =~ s=^\Q./==;
         my $locn = "$weblink/$fn" =~ s{([\s])}{sprintf("%%%02X",ord($1))}egr;  # poor man's urlencode (for '\s' only FTTB)
         push @mzrecs, "- $bc $locn $fn";
         }
      print "${p}mzrecs:\n";
      print( $p, $_, "\n" ) for @mzrecs;

      last;
      }

   # die unless $FsNmAbs dir tree size is < 1GB (or something more sophisticated)
   # do the needful to request mod_zip do its thing...
   #
   # Other items
   # handler_search_keys: when generating dir results, generate additional link
   #   (icon?) "download tree-zip" containing link to this page with some param
   #   that routes to handler_zipdownload.
   #
   # outside repo: modify nginx config to route

   print "${p}done\n";
   print $qy->ol( [ "Toverall: ".pr_et_us( time() - $tm_start ) ] ), "\n";
   print $qy->end_html;
   exit 0;
   }  # unimpl

sub search_keys_find_matches { my ($qy) = @_;
   my $isaset= $qy->Vars->{search_scope} // 'bavm'; # default to Book search
   my $skeys = $qy->Vars->{search_keys};
      # print(sprintf("%v04X", $skeys), "\n");
      # {
      # local $Data::Dumper::Useqq = 1;
      # print( decode_utf8(Dumper($skeys)) );
      # print( decode_utf8(encode_utf8($skeys)) );
      # }
      $skeys =~ s|&#9830;| |g; # remove nasty black-diamond-suit char that browser can inject when user hits browser-'back'.
      $skeys =~ s=${\q1}==g; # mimic qf file rename behavior
   my @search_keys = split( qr{[-\s,.:${\q2}]+}, $skeys ); # preserve: [+#]
   my $norm_search_terms = join(' ',@search_keys);
   my $pat = gen_re_match_all_anyorder( '\w_', @search_keys );

   # WIP to allow processes colliding on flock $semfh to try the set of sem files in random/shuffle order
   # this would also require separating find-phase from search phase as the following should seek to complete all find-phases before searching
   #
   # my @tlixs;
   # for my $ix (0 .. $#treelocns) {
   #    my ($isa,$fsroot,$webroot,$findtype) = @{$treelocns[$ix]}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
   #    next unless $isa =~ m=[$isaset]=;
   #    push @tlixs, $ix;
   #    }
   # @tlixs = shuffle(@tlixs);  requires  -->  use List::Util 'shuffle';
   # while( scalar @tlixs ) {
   #    for my $ix (0 .. $#tlixs) {
   #       for my $hr ( @tlixs ) {
   #          my ($isa,$fsroot,$webroot,$findtype) = @{$hr}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
   #          my $matches = search_treelocn($qy,$isa,$fsroot,$webroot,$findtype,$pat);
   #          if( $matches ) {
   #             $hr->{matches} = $matches;
   #
   #             }
   #          }
   #       }
   #    }

   my $su = $qy->self_url =~ s=\?.*==r;
   # print $su;
   my ($cands,$matches) = (0, 0);  # counters accumulated across all calls to search_treelocn
   for my $hr ( @treelocns ) {
      my ($isa,$fsroot,$webroot,$findtype) = @{$hr}{qw(isa fsroot webroot ft)}; # print "\nfsroot=$fsroot\n";
      next unless $isa =~ m=[$isaset]=;
      my ($ccnt, $mcnt); ($hr->{matches}, $ccnt, $mcnt) = search_treelocn($qy,$isa,$fsroot,$webroot,$findtype,$pat,$su);
      $cands   += $ccnt;
      $matches += $mcnt;
      }
   return ($norm_search_terms,$pat,$cands,$matches);
   }

sub handler_search_keys { my ($qy) = @_;
   my ($norm_search_terms,$pat,$cands,$matches) = search_keys_find_matches( $qy );
   print $qy->header;
   print $qy->start_html(
         -title=>$norm_search_terms,
         -base=>'true', -target=>'_blank',  # links on this page will open in new client (browser) tab/window.
         );
   print "$pat\n" if showRegex;
   # print $qy->Dump();
   print $qy->h3("Found",($matches||"no"),"matches of '$norm_search_terms' among",$cands,"candidates",$qy->a({href=>"/"},"new search")), "\n";
   for my $hr ( @treelocns ) {
      my $ms = $hr->{matches};
      if( scalar keys %{$ms} ) {
         my $cat = $hr->{cat};
         print $qy->a({href=>"#$cat"},$cat),"\n";
         }
      }
   for my $hr ( @treelocns ) {
      my $ms = $hr->{matches};
      if( scalar keys %{$ms} ) {
         my $anch = { id => $hr->{cat} };
         for my $yr (reverse sort keys %{$ms}) { # print "$yr:\n";
            print $qy->h3( $anch, $hr->{cat} . " &copy;".($yr || "<i>unknown</i>") ), "\n";
            $anch = {};
            print $qy->ol( map { $qy->li( $_ ); } sort @{$ms->{$yr}} ), "\n";
            }
         }
      }
   print $qy->h3( "Server response timing:" );
   print $qy->ol( [ "Toverall: ".pr_et_us( time() - $tm_start ), " Tfind: ".pr_et_us( $et_find ), " Tsrch: ".pr_et_us( $et_srch ) ] ), "\n";
   print $qy->end_html;
   exit 0;
   }
